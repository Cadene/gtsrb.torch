---
layout: post
title: 99.61% on gtsrb with Spatial Transformer Networks
comments: True
author: albanD
---

<!---# 99.61% on gtsrb with Spatial Transformer Networks-->

## Tl-dr

A couple of weeks ago [Google DeepMind](http://deepmind.com/index.html) released an awesome article called [Spatial Transformer Networks](http://arxiv.org/abs/1506.02025) aiming at boosting the geometric invariance of CNNs in a very elegant way.

This approach was so appealing to us at [Moodstocks](https://moodstocks.com) that we decided to implement it and see how it performs on a not-so-simple dataset called the GTSRB.

At the end of the day Spatial Transformer Networks enabled us to outperform the state-of-the-art with a much simpler pipeline (no jittering, no parallel networks, no fancy normalization techniques, ...)

## The GTSRB dataset

The GTSRB dataset (German Traffic Sign Recognition Benchmark) is provided by the Institut f√ºr Neuroinformatik group [here](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news). It was published for a competition held in 2011([results](http://benchmark.ini.rub.de/?section=gtsrb&subsection=results)). Images are spread across 43 different types of traffic signs and contains a total of 39209 train examples and 12630 test ones.

![dataset-samples](https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/traffic-signs.png)

We like this dataset a lot at Moodstocks: it's lightweight, yet hard enough to test new ideas. For the record, the contest winner achieved a 99,46% top-1 accuracy thanks to a committee of 25 networks and by using a bunch of augmentations and data normalization techniques.

## Spatial transformer
### Layer presentation
The goal of the spatial transformer [1] is to add a layer able to perform an explicit geometric transformation on an input. The parameters of the transformation are learnt thanks to the standard backpropagation algorithm.

![st-structure](https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/spatial-transformer-structure.png)

The layer is composed of 3 elements:

* The *localization network* takes the original image as an input and outputs the parameters of the transformation we want to apply.
* The *grid generator* generates a grid of coordinates in the input image corresponding to each pixel from the output image.
* The *sampler* generates the output image using the grid given by the grid generator.

As an example, here is what you get after training a network whose first layer is a ST:

![st-exemple](https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/st-mnist.png)

On the left you see the input image. In the middle you see which part of the input image is sampled. On the right you see the Spatial Transformer output image.

### Implementation details

We leveraged the grid generator and the sampler coded by Maxime Oquab in his great [stnbhwd](https://github.com/qassemoquab/stnbhwd) project.
We added a module placed between the localization network and the grid generator to let us restrict the possible transformations.

Using these modules, creating a spatial transformer layer using torch logic is as easy as:

{% gist albanD/954021a4be9e1ccab753 %}

## Overview of the gtsrb.torch repo

The full code is available on the [Moodstocks Github](https://github.com/moodstocks/gtsrb.torch). We designed it to let you perform a large range of tests on the dataset, including the spatial transformer. For all details about the architecture, feel free to check out the [docs](https://github.com/moodstocks/gtsrb.torch#content) in the repo.

You can do a lot with this code, but for the sake of the blogpost let's focus on a few basic parameters:

* `--cnn` to control the shape of the CNN used for the classification task
* `--st` and `--locnet` to add a spatial transformer and control the localization network's shape
* `--rot`, `--sca` and `--tra` to restrict the transformations (rotation, scale, translation) that the spatial transformer can learn.

For example, the command line to run our best configuration is:

``` bash
# This takes ~5 min per epoch and 1750MB ram on a Titan X
luajit main.lua -n -1 --st --net idsia_net.lua --cnn 150,200,300,350 --locnet 200,300,200 --locnet3 150,150,150 -e 14
```

## Results
### Baseline
The Idsia guys won the contest back in 2011 with a 99.46% top-1 accuracy. 

To achieve this result, they:

* used a network made of 3 convolutional and 2 fully connected layers,
* designed 4 different normalization techniques to get 5 versions of the original dataset,
* jittered (scaling, translation and rotation) the training images,
* trained 5 networks per version of the dataset (25 total networks),
* averaged their outputs over the 25 networks.

### Our achievement using a Spatial Transformer network
Starting from their network of 3 convolutional and 2 fully connected layers, we got a 99.61% top-1 accuracy after only 14 epochs simply by adding 2 ST layers. That's it!

We can also see the effect of the first spatial transformer using our [plotting script](https://github.com/moodstocks/gtsrb.torch/tree/master/#plotting-script) (on the left you see the input image, on the right the output of the Spatial Transformer):
![plot-ex](https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/plot-example.png)

## Conclusion
Spatial Transformer Networks are a very appealing way to boost the geometric invariance of CNNs and hence improve your top-1 accuracy. We managed to outperform the state-of-the-art on a not-so-simple dataset (GTSRB) while drastically simplifying the pipeline. Feel free to use [our code](https://github.com/moodstocks/gtsrb.torch) to reproduce our results or even get better ones: we provide a fancy way to [mass benchmark](https://github.com/Moodstocks/gtsrb.torch/blob/master/docs/bench.md) configurations to help you do that. Have fun! 

1. *Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu*, Spatial Transformer Networks [[arxiv]](http://arxiv.org/abs/1506.02025)
2. *P. Sermanet, Y. LeCun*, Traffic sign recognition with multi-scale Convolutional Networks [[link]](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf)
3. *D. Ciresan, U. Meier, J. Masci, J. Schmidhuber*, Multi-column deep neural network for traffic sign classification [[link]](http://people.idsia.ch/~juergen/nn2012traffic.pdf)
